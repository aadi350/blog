<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Graph Neural Network | Aadis Blog</title>
<meta name="keywords" content="">
<meta name="description" content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.">
<meta name="author" content="">
<link rel="canonical" href="https://aadi-blogs.web.app/blog/graph-neural-network/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css" integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://aadi-blogs.web.app/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://aadi-blogs.web.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://aadi-blogs.web.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://aadi-blogs.web.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://aadi-blogs.web.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="The Graph Neural Network" />
<meta property="og:description" content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aadi-blogs.web.app/blog/graph-neural-network/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2022-06-25T06:38:42-04:00" />
<meta property="article:modified_time" content="2022-06-25T06:38:42-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="The Graph Neural Network"/>
<meta name="twitter:description" content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings."/>
<meta name="twitter:site" content="@https://twitter.com/cats"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://aadi-blogs.web.app/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Graph Neural Network",
      "item": "https://aadi-blogs.web.app/blog/graph-neural-network/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Graph Neural Network",
  "name": "The Graph Neural Network",
  "description": "The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.\n(If you need a refresher on deep learning, see here)\nThe idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node\u0026rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.",
  "keywords": [
    
  ],
  "articleBody": " The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.\n(If you need a refresher on deep learning, see here)\nThe idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node’s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.\nThis task is further complicated by the fact that typical deep-learning approaches (Convolutional and Recurrent Neural Networks) expect some form of data structured in the Euclidean plane (images or sequences of text). Hence, a completely new way of utilizing deep, multi-layer perceptrons was needed.\nBefore we go further, here are two concepts that are fairly significant to the field: Inductive and Transductive Learning\nInductive learning is what you’d think about as typical, supervised machine learning. This is where a model learns general rules from observed training data, which are then applied to test cases which are unseen during training. Although the model is exposed to a restricted scope of training data, it is expected to generalize by learning latent pattern present in a feature-target relationship.\nTranductive learning uses both the training and testing data during the learning phase. In this case, the model is, for example, aware of test-node in a graph, but attempts to find information in the combined dataset for later use in predicting the unlabelled data points\nNow that we’ve gotten definitions out of the way, we need to define some method, or set of functions by which our deep embeddings can be generated. Additionally, these embeddings need be permutation invariant and equivariant. (This is why we can’t simply feed the adjacency matrix into a neural network; The order of the nodes in the matrix would impact the actual solutions approximated by the network, and the number of parameters in the network would severely outstrip the number of nodes thereby inducing inherent instability in training and result in overfitting)\nMathematically, if $\\bold{P}$ is the permutation matrix:\n$$ f(\\textbf{PAP}^T) = f(\\textbf{A}) $$ $$ f(\\textbf{PAP}^T) = \\textbf{P}f(\\textbf{A}) $$\nPermutation invariance means that the output doesn’t depend on how the rows and columns are ordered (multiple adjacency matrices can represent the same graph). Permutation equivariance means that the output of $f$ is permuted in a consistent way when $\\bold{A}$ is permuted.\nThe Basic Neural Network Model I won’t go through the motivations of how the GNN materialized, but I can think of it as a generalization of convolutions to non-Euclidean data. The idea is to have a vector $\\bold{h}_u$, or hidden-state for each node $u$, which is updated iteratively by gaining information from its neighbourhood. (This is known as neural message passing, because the updates from the neighbourhood is received via a nonlinear activation function wrapping a neural network)\nThe way we can mathematically generalize this notion is by the following:\nWe generate a node embedding $\\bold{h}_u^{(k+1)}$ (for node $u$ from its $k+1$-hop neighbourhood), by applying some function $U$, to both its own embedding from the previous ($k^{th}$) iteration, as well as from the aggregate of its neighbours’ embeddings. Less confusingly:\n$$ \\bold{h}_u^{k+1} = U^k(\\bold{h}_u^k, AGG^k({\\bold{h}_v^k)), \\forall v\\in\\mathcal{N}_u} $$\nwhere $\\mathcal{N}_u$ are all the nodes in the neighbourhood of node $u$. To put it more simply, we can think of the aggregate information as a neural “message” from the node neighbours, passed through the update function, along with the node’s previous state at the previous iteration.\nAt each iteration, a node learns more about its wider neigbourhood, and as such, the above is not iterated to convergence, but is instead iterated for a fixed, pre-determined $K$ set of times. More concretely, at iteration $k=1$, a node has information from its immediate, one-hop neighbourhood (nodes that can be reached using a path length of one in the graph). In general, after $k$ iterations, every node contains information about its $k$-hop neighbourhood.\nThis information is composed both structural and feature-based components. Structure would comprise encoded information about the density and connectivity of the node’s neighbourhood, while feature-based information aggregation would be analagous to the operation of convolutional kernels in a pixel-neighbourhood.\nThinking About Implementation In order to concretize the above formulation, we need to define actual functions for the update and aggregate step. As given in the 2008 paper, the aggregate function is given by:\n$$ \\boldsymbol{m}_{N(u)} = \\sum_u\\boldsymbol{h}_v, \\forall u \\in \\mathcal{N}_u $$\nand the update function is given by:\n$$ U = \\sigma\\left(\\boldsymbol{W}_uh_u+\\boldsymbol{W}_n\\boldsymbol{m}_n\\right) $$\nTypes of Aggregate Functions Taking the sum of node features is highly sensitive to the number of nodes in a given neighbourhood, as such different ways of normalizing the aggregate function have been proposed:\n1. Neighbourhood Normalization A simple way to account for varying ranges of node degrees is to simply normalize the sum of node features by the number of nodes. $$ \\boldsymbol{M}_\\mathcal{N(u)}=\\frac{\\sum_v \\boldsymbol{h}_v}{|\\mathcal{N}(u)|} $$\nOthers, such as the symmetric normalization used for citation networks, idea being that high-degree nodes may not be useful for finding communities, since they are cited across many diverse subfields. $$ \\boldsymbol{M}_\\mathcal{N(u)}=\\sum_v\\frac{\\boldsymbol{h}_v}{\\sqrt{|\\mathcal{N(u)}||\\mathcal{N(v)}|}} $$\n2. Graph Convolutions This was proposed in this paper in 2015 and is based on the Fourier decomposition of graph-signals. The idea is that eigenvectors of the graph Laplacian is associated with a corresponding eigenvalue analagous to the complex exponential at a certain frequency. The message-passing function is therefore defined as: $$ \\boldsymbol{h}_u^k=,\\sigma\\left(\\boldsymbol{W}\\sum_v\\frac{\\boldsymbol{h}_v}{\\sqrt{|\\mathcal{N(u)}||\\mathcal{N(v)}|}}\\right) \\in\\mathcal{N}(u)\\cup {u} $$ It is of note that using the above formulation, we also use the concept of self-loops, in order to eliminate an explicit update step, where aggregation is taken over the joing set $\\mathcal{N}\\cup {u}$\n3. Neighbourhood Pooling Any permutation-invariant function which maps to a single embedding is suitable for the aggregate function. One way to do this is by using an arbitrarily deep multi-layer perceptron MLP with some trainable parameters $t$. For example, using a sum function: $$ \\boldsymbol{m}_{\\mathcal{N}(u)}=\\text{MLP}_t \\left(\\sum_v \\text{MLP}_p (\\boldsymbol{h}_v)\\right) $$\n4. Attention This is possibly the most hyped topic in machine learning over the past 5 years since the 2015 paper and the subsequent explosion of multi-head attention (known as transformers). The fundamental predicate is in weighting each neighbour based on their influence (left up to interpretation) during the aggregation step. For example:\n$$ \\bold{m}_{\\mathcal{N(u)}}=\\sum_v \\alpha _{u,v}h_v, v\\in\\mathcal{N(u)} $$ You can see here for the original Graph-attention paper to see how the attention mechanism was defined.\nGAT is shown to work well with graph data, where others have applied differing ways of aggregating the attention mechanisms\n# basic import dgl import dgl.function as fn import torch import torch.nn as nn from torch.nn import init import torch.nn.functional as F class GraphConv(nn.Module): def __init__(self, in_feat, out_feat, k=1): super(GraphConv, self).__init__() self.fc = nn.Linear(in_feat, out_feat, bias=True) self.k = k nn.init.xavier_uniform_(self.fc.weight) nn.init.zeros_(self.fc.bias) def forward(self, graph, feat): msg_func = fn.copy_u(\"h\", \"m\") degs = graph.in_degrees().float().clamp(min=1) norm = torch.pow(degs, -0.5) norm = norm.to(feat.device).unsqueeze(1) # hop-step for _ in range(self.k): graph.ndata['h'] = feat graph.update_all(msg_func, fn.sum('m', 'h')) feat = graph.ndata.pop('h') feat = feat * norm return self.fc(feat) # GraphSAGE import dgl.function as fn class SAGEConv(nn.Module): \"\"\"Graph convolution module used by the GraphSAGE model. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. \"\"\" def __init__(self, in_feat, out_feat): super(SAGEConv, self).__init__() # A linear submodule for projecting the input and neighbor feature to the output. self.linear = nn.Linear(in_feat * 2, out_feat) def forward(self, g, h): \"\"\"Forward computation Parameters ---------- g : Graph The input graph. h : Tensor The input node feature. \"\"\" with g.local_scope(): g.ndata['h'] = h # update_all is a message passing API. g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N')) h_N = g.ndata['h_N'] h_total = torch.cat([h, h_N], dim=1) return self.linear(h_total) Types of Update Functions While the aggregate function defines how the data is treated as it arrives at each node, the update function defines where the data moves between nodes. A common issue is over-smoothing, making it impossible to build deeper models. This is due to the fact that the common message-passing paradigm is essentially a low-pass filter over the graph. As the signal is propagated throughout the graph, the high-frequency patterns get lost in the constancy of the low frequencies. This results in the updated node representations which depend too strongly on incoming message from neighbours, at expense of node embeddings from previous neighbours.\nTwo ways of addressing this are: skip connections and gated updates.\n1. Skip Connections This is analagous to drop-out and skip-connections from computer vision. Essentially, only updates from certain nodes are allowed to arrive at a given node. This is implemented by concatenating the output of the update function with the node’s previous-layer representation (like GraphSAGE). Another interpretation is by linearly interpolating between the current and new node values to achieve updated state:\n$$ \\text{UPDATE} _\\text{interpolate}(\\boldsymbol{h}_u, \\boldsymbol{m} _\\mathcal{N(u)})=\\alpha\\circ\\text{UPDATE}(\\boldsymbol{h}_u, \\boldsymbol{m} _\\mathcal{N(u)})+\\alpha_2\\circ\\boldsymbol{h}_u $$\n$\\alpha_1, \\alpha_2\\in [0,1]^d$ are gating vectors s.t. $\\alpha_2 = 1-\\alpha_1$. Updated representation is a linear interpolation between previous embedding and (vanilla) updated embedding.\nSkip-connections address over-smoothing and numerical stability during optimization.\n2. Gated Updates If skip-connections are analagous to dropout in Convolutional Neural Networks, gated updates are analagous to the Gated-Recurrent Unit in the RNN world. Here, an aggregation function receives an observation from its neighbours, which is then used to update a hidden node state. In this case, we can apply basic RNN/GRU logic: $$ \\bold{h} _u^{(k)}=\\text{GRU}(\\bold{h} _u^{k-1}, \\bold{m} _{\\mathcal{N}(u)}^k) $$\nclass GatedGraphConv(nn.Module): def __init__(self, in_feats, out_feats, n_steps, n_etypes, bias=True): super(GatedGraphConv, self).__init__() self.in_feats = in_feats self.out_feats = out_feats self.n_steps = n_steps self.n_etypes = n_etypes self.linear_layers = nn.ModuleList( [nn.Linear(out_feats, out_feats) for _ in range(n_etypes)] ) self.gru = nn.GRUCell(out_feats, out_feats, bias=bias) gain = init.calculate_gain('relu') self.gru.reset_parameters() for linear in self.linear_layers: init.xavier_normal_(linear.weight, gain=gain) init.zeros_(linear.bias) def forward(self, graph, feat, etypes=None): with graph.local_scope(): zero_pad = feat.new_zeros( (feat.shape[0], self.out_feats - feat.shape[1]) ) feat = torch.cat([feat, zero_pad],-1) for _ in range(self.steps): graph.ndata['h'] = feat for i in range(self.n_etypes): eids = torch.nonzero( etypes==i, as_tuple=False ).view(-1).type(graph.idtype) if len(eids) \u003e 0: graph.apply_edges( lambda edges: { 'W_e*h': self.linear_layers[i](edges.src['h']) } ) graph.update_all(fn.copy_e('W_e*h', 'm'), fn.sum('m', 'a')) a = graph.ndata.pop('a') feat = self.gru(a, feat) return feat g=GatedGraphConv(10,2, 2,3) Notes on MPNNs The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a node’s neighborhood into a dense vector embedding. These node embeddings can then be fed to downstream machine learning systems and aid in tasks such as node classification, clustering, and link prediction. MPNNs can additionally generalize to much larger graphs (see here)\nMPNNs Limits Message-passing has linear time complexity (see Breaking the Limits of Message Passing Graph Neural Networks). This may be a limit depending on what architecture it is compared to. (For example, even basic CNNs usually are not linear).\nIt it theoretically impractical to make an MPNN more powerful in terms of the 1-WL test. The 1-WL test is a standard measure of the ability of a particular model to differentiate between non-isomorphic graphs. (Graphs are isomorphic if a relabelling of one graph results in another).\n1-WL graphs (MPNNs) cannot count the number of cycles, triangles and other strucutral features, informative for some social and/or chemical graphs (see here).\nHowever, heres is an interesting paper that practically superceeds the expressive power of the 1-WL test (http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf). Additionally, more “features” of the graph have been proposed to potentially increase its ability in terms of 1-WL, such as adding trainable weights for:\nDistance between nodes (Deferard 2016) Connected node features GAT Edge-features (Bresson and Laurent 2018) ",
  "wordCount" : "1898",
  "inLanguage": "en",
  "datePublished": "2022-06-25T06:38:42-04:00",
  "dateModified": "2022-06-25T06:38:42-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aadi-blogs.web.app/blog/graph-neural-network/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Aadis Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://aadi-blogs.web.app/favicon.png"
    }
  }
}
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>

</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://aadi-blogs.web.app/" accesskey="h" title="Aadis Blog (Alt + H)">Aadis Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      The Graph Neural Network
    </h1>
    <div class="post-meta"><span title='2022-06-25 06:38:42 -0400 AST'>June 25, 2022</span>

</div>
  </header> 
  <div class="post-content">
<p>The Graph Neural Network (GNN) was proposed (<a href="https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&amp;context=infopapers">Scarselli, 2008</a>) as a general framework for defining deep neural networks on graph data.</p>
<p>(If you need a refresher on deep learning, see <a href="https://www.youtube.com/watch?v=tutlI9YzJ2g">here</a>)</p>
<p>The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.</p>
<p>This task is further complicated by the fact that typical deep-learning approaches (Convolutional and Recurrent Neural Networks) expect some form of data structured in the Euclidean plane (images or sequences of text). Hence, a completely new way of utilizing deep, multi-layer perceptrons was needed.</p>
<!-- raw HTML omitted -->
<p>Before we go further, here are two concepts that are fairly significant to the field: Inductive and Transductive Learning</p>
<p><strong>Inductive learning</strong> is what you&rsquo;d think about as typical, supervised machine learning. This is where a model learns general rules from observed training data, which are then applied to test cases which are unseen during training. Although the model is exposed to a restricted scope of training data, it is expected to generalize by learning latent pattern present in a feature-target relationship.</p>
<p><strong>Tranductive learning</strong> uses both the training and testing data during the learning phase. In this case, the model is, for example, aware of test-node in a graph, but attempts to find information in the combined dataset for later use in predicting the unlabelled data points</p>
<p>Now that we&rsquo;ve gotten definitions out of the way, we need to define some method, or set of functions by which our deep embeddings can be generated. Additionally, these embeddings need be permutation invariant and equivariant. (This is why we can&rsquo;t simply feed the adjacency matrix into a neural network; The order of the nodes in the matrix would impact the actual solutions approximated by the network, and the number of parameters in the network would severely outstrip the number of nodes thereby inducing inherent instability in training and result in overfitting)</p>
<p>Mathematically, if $\bold{P}$ is the permutation matrix:</p>
<p>$$
f(\textbf{PAP}^T) = f(\textbf{A})
$$
$$
f(\textbf{PAP}^T) = \textbf{P}f(\textbf{A})
$$</p>
<p>Permutation invariance means that the output doesn&rsquo;t depend on how the rows and columns are ordered (multiple adjacency matrices can represent the same graph). Permutation equivariance means that the output of $f$ is permuted in a consistent way when $\bold{A}$ is permuted.</p>
<h2 id="the-basic-neural-network-model">The Basic Neural Network Model<a hidden class="anchor" aria-hidden="true" href="#the-basic-neural-network-model">#</a></h2>
<p>I won&rsquo;t go through the motivations of how the GNN materialized, but I can think of it as a generalization of convolutions to non-Euclidean data. The idea is to have a vector $\bold{h}_u$, or hidden-state for each node $u$, which is updated iteratively by gaining information from its neighbourhood. (This is known as <em>neural message passing</em>, because the updates from the neighbourhood is received via a nonlinear activation function wrapping a neural network)</p>
<p>The way we can mathematically generalize this notion is by the following:</p>
<p>We generate a node embedding $\bold{h}_u^{(k+1)}$ (for node $u$ from its $k+1$-hop neighbourhood), by applying some function $U$, to both its own embedding from the previous ($k^{th}$) iteration, as well as from the aggregate of its neighbours&rsquo; embeddings. Less confusingly:</p>
<p>$$
\bold{h}_u^{k+1} = U^k(\bold{h}_u^k, AGG^k({\bold{h}_v^k)), \forall v\in\mathcal{N}_u}
$$</p>
<p>where $\mathcal{N}_u$ are all the nodes in the neighbourhood of node $u$. To put it more simply, we can think of the aggregate information as a neural &ldquo;message&rdquo; from the node neighbours, passed through the update function, along with the node&rsquo;s previous state at the previous iteration.</p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->
<p>At each iteration, a node learns more about its wider neigbourhood, and as such, the above is not iterated to convergence, but is instead iterated for a fixed, pre-determined $K$ set of times. More concretely, at iteration $k=1$, a node has information from its immediate, one-hop neighbourhood (nodes that can be reached using a path length of one in the graph). In general, after $k$ iterations, every node contains information about its $k$-hop neighbourhood.</p>
<p>This information is composed both structural and feature-based components. Structure would comprise encoded information about the density and connectivity of the node&rsquo;s neighbourhood, while feature-based information aggregation would be analagous to the operation of convolutional kernels in a pixel-neighbourhood.</p>
<h1 id="thinking-about-implementation">Thinking About Implementation<a hidden class="anchor" aria-hidden="true" href="#thinking-about-implementation">#</a></h1>
<p>In order to concretize the above formulation, we need to define actual functions for the update and aggregate step. As given in the <a href="https://ieeexplore.ieee.org/abstract/document/4700287">2008 paper</a>, the aggregate function is given by:</p>
<p>$$
\boldsymbol{m}_{N(u)} = \sum_u\boldsymbol{h}_v, \forall u \in \mathcal{N}_u
$$</p>
<p>and the update function is given by:</p>
<p>$$
U = \sigma\left(\boldsymbol{W}_uh_u+\boldsymbol{W}_n\boldsymbol{m}_n\right)
$$</p>
<h2 id="types-of-aggregate-functions">Types of Aggregate Functions<a hidden class="anchor" aria-hidden="true" href="#types-of-aggregate-functions">#</a></h2>
<p>Taking the sum of node features is highly sensitive to the number of nodes in a given neighbourhood, as such different ways of normalizing the aggregate function have been proposed:</p>
<h3 id="1-neighbourhood-normalization">1. Neighbourhood Normalization<a hidden class="anchor" aria-hidden="true" href="#1-neighbourhood-normalization">#</a></h3>
<p>A simple way to account for varying ranges of node degrees is to simply normalize the sum of node features by the number of nodes.
$$
\boldsymbol{M}_\mathcal{N(u)}=\frac{\sum_v \boldsymbol{h}_v}{|\mathcal{N}(u)|}
$$</p>
<p>Others, such as the symmetric normalization used for citation networks, idea being that high-degree nodes may not be useful for finding communities, since they are cited across many diverse subfields.
$$
\boldsymbol{M}_\mathcal{N(u)}=\sum_v\frac{\boldsymbol{h}_v}{\sqrt{|\mathcal{N(u)}||\mathcal{N(v)}|}}
$$</p>
<h3 id="2-graph-convolutions">2. Graph Convolutions<a hidden class="anchor" aria-hidden="true" href="#2-graph-convolutions">#</a></h3>
<p>This was proposed in <a href="https://arxiv.org/abs/1609.02907">this paper</a> in 2015 and is based on the Fourier decomposition of graph-signals. The idea is that eigenvectors of the graph Laplacian is associated with a corresponding eigenvalue analagous to the complex exponential at a certain frequency. The message-passing function is therefore defined as:
$$
\boldsymbol{h}_u^k=,\sigma\left(\boldsymbol{W}\sum_v\frac{\boldsymbol{h}_v}{\sqrt{|\mathcal{N(u)}||\mathcal{N(v)}|}}\right) \in\mathcal{N}(u)\cup {u}
$$
It is of note that using the above formulation, we also use the concept of self-loops, in order to eliminate an explicit update step, where aggregation is taken over the joing set $\mathcal{N}\cup {u}$</p>
<h3 id="3-neighbourhood-pooling">3. Neighbourhood Pooling<a hidden class="anchor" aria-hidden="true" href="#3-neighbourhood-pooling">#</a></h3>
<p>Any permutation-invariant function which maps to a single embedding is suitable for the aggregate function. One way to do this is by using an arbitrarily deep multi-layer perceptron MLP with some trainable parameters $t$. For example, using a sum function:
$$
\boldsymbol{m}_{\mathcal{N}(u)}=\text{MLP}_t \left(\sum_v \text{MLP}_p (\boldsymbol{h}_v)\right)
$$</p>
<h3 id="4-attention">4. Attention<a hidden class="anchor" aria-hidden="true" href="#4-attention">#</a></h3>
<p>This is possibly the most hyped topic in machine learning over the past 5 years since the <a href="https://proceedings.neurips.cc/paper/2015/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html">2015 paper</a> and the subsequent explosion of multi-head attention (known as transformers). The fundamental predicate is in weighting each neighbour based on their influence (left up to interpretation) during the aggregation step. For example:</p>
<p>$$
\bold{m}_{\mathcal{N(u)}}=\sum_v \alpha _{u,v}h_v, v\in\mathcal{N(u)}
$$
You can see <a href="">here</a> for the original Graph-attention paper to see how the attention mechanism was defined.</p>
<p>GAT is shown to work well with graph data, where others have applied differing ways of aggregating the attention mechanisms</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># basic</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> dgl 
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> dgl.function <span style="color:#66d9ef">as</span> fn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.nn <span style="color:#f92672">import</span> init
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F 
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GraphConv</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_feat, out_feat, k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        super(GraphConv, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_feat, out_feat, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>k <span style="color:#f92672">=</span> k
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform_(self<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>weight)
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>zeros_(self<span style="color:#f92672">.</span>fc<span style="color:#f92672">.</span>bias)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, graph, feat):
</span></span><span style="display:flex;"><span>        msg_func <span style="color:#f92672">=</span> fn<span style="color:#f92672">.</span>copy_u(<span style="color:#e6db74">&#34;h&#34;</span>, <span style="color:#e6db74">&#34;m&#34;</span>)
</span></span><span style="display:flex;"><span>        degs <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>in_degrees()<span style="color:#f92672">.</span>float()<span style="color:#f92672">.</span>clamp(min<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        norm <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>pow(degs, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>        norm <span style="color:#f92672">=</span> norm<span style="color:#f92672">.</span>to(feat<span style="color:#f92672">.</span>device)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># hop-step</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>k):
</span></span><span style="display:flex;"><span>            graph<span style="color:#f92672">.</span>ndata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat
</span></span><span style="display:flex;"><span>            graph<span style="color:#f92672">.</span>update_all(msg_func, fn<span style="color:#f92672">.</span>sum(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;h&#39;</span>))
</span></span><span style="display:flex;"><span>            feat <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>ndata<span style="color:#f92672">.</span>pop(<span style="color:#e6db74">&#39;h&#39;</span>)
</span></span><span style="display:flex;"><span>            feat <span style="color:#f92672">=</span> feat <span style="color:#f92672">*</span> norm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>fc(feat)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># GraphSAGE</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> dgl.function <span style="color:#66d9ef">as</span> fn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SAGEConv</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Graph convolution module used by the GraphSAGE model.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    in_feat : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Input feature size.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    out_feat : int
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Output feature size.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_feat, out_feat):
</span></span><span style="display:flex;"><span>        super(SAGEConv, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># A linear submodule for projecting the input and neighbor feature to the output.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_feat <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, out_feat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, g, h):
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;&#34;&#34;Forward computation
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        Parameters
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        ----------
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        g : Graph
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            The input graph.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        h : Tensor
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">            The input node feature.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> g<span style="color:#f92672">.</span>local_scope():
</span></span><span style="display:flex;"><span>            g<span style="color:#f92672">.</span>ndata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> h
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># update_all is a message passing API.</span>
</span></span><span style="display:flex;"><span>            g<span style="color:#f92672">.</span>update_all(message_func<span style="color:#f92672">=</span>fn<span style="color:#f92672">.</span>copy_u(<span style="color:#e6db74">&#39;h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), reduce_func<span style="color:#f92672">=</span>fn<span style="color:#f92672">.</span>mean(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;h_N&#39;</span>))
</span></span><span style="display:flex;"><span>            h_N <span style="color:#f92672">=</span> g<span style="color:#f92672">.</span>ndata[<span style="color:#e6db74">&#39;h_N&#39;</span>]
</span></span><span style="display:flex;"><span>            h_total <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([h, h_N], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linear(h_total)
</span></span></code></pre></div><h1 id="types-of-update-functions">Types of Update Functions<a hidden class="anchor" aria-hidden="true" href="#types-of-update-functions">#</a></h1>
<p>While the aggregate function defines <em>how</em> the data is treated as it arrives at each node, the update function defines <em>where</em> the data moves between nodes. A common issue is over-smoothing, making it impossible to build deeper models. This is due to the fact that the common message-passing paradigm is <a href="https://openreview.net/forum?id=-qh0M9XWxnv">essentially a low-pass filter over the graph</a>. As the signal is propagated throughout the graph, the high-frequency patterns get lost in the constancy of the low frequencies. This results in the updated node representations which depend too strongly on incoming message from neighbours, at expense of node embeddings from previous neighbours.</p>
<p>Two ways of addressing this are: skip connections and gated updates.</p>
<h2 id="1-skip-connections">1. Skip Connections<a hidden class="anchor" aria-hidden="true" href="#1-skip-connections">#</a></h2>
<p>This is analagous to drop-out and skip-connections from computer vision. Essentially, only updates from certain nodes are allowed to arrive at a given node. This is implemented by concatenating the  output of the update function with the node&rsquo;s previous-layer representation (like GraphSAGE). Another interpretation is by linearly interpolating between the current and new node values to achieve updated state:</p>
<p>$$
\text{UPDATE} _\text{interpolate}(\boldsymbol{h}_u, \boldsymbol{m} _\mathcal{N(u)})=\alpha\circ\text{UPDATE}(\boldsymbol{h}_u, \boldsymbol{m} _\mathcal{N(u)})+\alpha_2\circ\boldsymbol{h}_u
$$</p>
<p>$\alpha_1, \alpha_2\in [0,1]^d$ are gating vectors s.t. $\alpha_2 = 1-\alpha_1$. Updated representation is a linear interpolation between previous embedding and (vanilla) updated embedding.</p>
<p>Skip-connections address over-smoothing and numerical stability during optimization.</p>
<h2 id="2-gated-updates">2. Gated Updates<a hidden class="anchor" aria-hidden="true" href="#2-gated-updates">#</a></h2>
<p>If skip-connections are analagous to dropout in Convolutional Neural Networks, gated updates are analagous to the Gated-Recurrent Unit in the RNN world. Here, an aggregation function receives an observation from its neighbours, which is then used to update a hidden node state. In this case, we can apply basic RNN/GRU logic:
$$
\bold{h} _u^{(k)}=\text{GRU}(\bold{h} _u^{k-1}, \bold{m} _{\mathcal{N}(u)}^k)
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GatedGraphConv</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_feats, out_feats, n_steps, n_etypes, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        super(GatedGraphConv, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>in_feats <span style="color:#f92672">=</span> in_feats
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out_feats <span style="color:#f92672">=</span> out_feats
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_steps <span style="color:#f92672">=</span> n_steps
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_etypes <span style="color:#f92672">=</span> n_etypes
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear_layers <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList(
</span></span><span style="display:flex;"><span>            [nn<span style="color:#f92672">.</span>Linear(out_feats, out_feats) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(n_etypes)]
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gru <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>GRUCell(out_feats, out_feats, bias<span style="color:#f92672">=</span>bias)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        gain <span style="color:#f92672">=</span> init<span style="color:#f92672">.</span>calculate_gain(<span style="color:#e6db74">&#39;relu&#39;</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>gru<span style="color:#f92672">.</span>reset_parameters()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> linear <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>linear_layers:
</span></span><span style="display:flex;"><span>            init<span style="color:#f92672">.</span>xavier_normal_(linear<span style="color:#f92672">.</span>weight, gain<span style="color:#f92672">=</span>gain)
</span></span><span style="display:flex;"><span>            init<span style="color:#f92672">.</span>zeros_(linear<span style="color:#f92672">.</span>bias)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, graph, feat, etypes<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> graph<span style="color:#f92672">.</span>local_scope():
</span></span><span style="display:flex;"><span>            zero_pad <span style="color:#f92672">=</span> feat<span style="color:#f92672">.</span>new_zeros(
</span></span><span style="display:flex;"><span>                (feat<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], self<span style="color:#f92672">.</span>out_feats <span style="color:#f92672">-</span> feat<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            feat <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([feat, zero_pad],<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>steps):
</span></span><span style="display:flex;"><span>                graph<span style="color:#f92672">.</span>ndata[<span style="color:#e6db74">&#39;h&#39;</span>] <span style="color:#f92672">=</span> feat
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>n_etypes):
</span></span><span style="display:flex;"><span>                    eids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nonzero(
</span></span><span style="display:flex;"><span>                        etypes<span style="color:#f92672">==</span>i, as_tuple<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>                    )<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>type(graph<span style="color:#f92672">.</span>idtype)
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">if</span> len(eids) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                        graph<span style="color:#f92672">.</span>apply_edges(
</span></span><span style="display:flex;"><span>                            <span style="color:#66d9ef">lambda</span> edges: {
</span></span><span style="display:flex;"><span>                                <span style="color:#e6db74">&#39;W_e*h&#39;</span>: self<span style="color:#f92672">.</span>linear_layers[i](edges<span style="color:#f92672">.</span>src[<span style="color:#e6db74">&#39;h&#39;</span>])
</span></span><span style="display:flex;"><span>                            }
</span></span><span style="display:flex;"><span>                        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            graph<span style="color:#f92672">.</span>update_all(fn<span style="color:#f92672">.</span>copy_e(<span style="color:#e6db74">&#39;W_e*h&#39;</span>, <span style="color:#e6db74">&#39;m&#39;</span>), fn<span style="color:#f92672">.</span>sum(<span style="color:#e6db74">&#39;m&#39;</span>, <span style="color:#e6db74">&#39;a&#39;</span>))
</span></span><span style="display:flex;"><span>            a <span style="color:#f92672">=</span> graph<span style="color:#f92672">.</span>ndata<span style="color:#f92672">.</span>pop(<span style="color:#e6db74">&#39;a&#39;</span>)
</span></span><span style="display:flex;"><span>            feat  <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>gru(a, feat)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> feat
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>g<span style="color:#f92672">=</span>GatedGraphConv(<span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>)
</span></span></code></pre></div><h2 id="notes-on-mpnns">Notes on MPNNs<a hidden class="anchor" aria-hidden="true" href="#notes-on-mpnns">#</a></h2>
<p>The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the
high-dimensional information about a node’s neighborhood into a dense vector embedding. These
node embeddings can then be fed to downstream machine learning systems and aid in tasks such as
node classification, clustering, and link prediction. MPNNs can additionally generalize to much larger graphs (see <a href="">here</a>)</p>
<h3 id="mpnns-limits">MPNNs Limits<a hidden class="anchor" aria-hidden="true" href="#mpnns-limits">#</a></h3>
<p>Message-passing has linear time complexity (see <a href="http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf">Breaking the Limits of Message Passing Graph Neural Networks</a>). This may be a limit depending on what architecture it is compared to. (For example, even basic CNNs usually are not linear).</p>
<p>It it theoretically impractical to make an MPNN more powerful in terms of the 1-WL test. The <a href="https://arxiv.org/pdf/2201.07083.pdf">1-WL test</a> is a standard measure of the ability of a particular model to differentiate between non-isomorphic graphs. (Graphs are isomorphic if a relabelling of one graph results in another).</p>
<p>1-WL graphs (MPNNs) cannot count the number of cycles, triangles and other strucutral features, informative for some social and/or chemical graphs (see <a href="https://arxiv.org/pdf/2201.07083.pdf">here</a>).</p>
<p>However, heres is an interesting paper that practically superceeds the expressive power of the 1-WL test (<a href="http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf)">http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf)</a>. Additionally, more &ldquo;features&rdquo; of the graph have been proposed to potentially increase its ability in terms of 1-WL, such as adding trainable weights for:</p>
<ul>
<li>Distance between nodes (Deferard 2016)</li>
<li>Connected node features GAT</li>
<li>Edge-features (Bresson and Laurent 2018)</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://aadi-blogs.web.app/">Aadis Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
